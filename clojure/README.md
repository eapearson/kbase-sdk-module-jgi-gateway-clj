# Clojure JGI Gateway

The JGI Gateway is a provides a kbase-facing interface to the jgi-nersc query api.

## Developing

### Prerequisites

#### Install java

sudo add-apt-repository ppa:openjdk-r/ppa
sudo apt-get install openjdk-8-jdk
sudo update-ca-certificates -f
https://leiningen.org/
wget https://raw.githubusercontent.com/technomancy/leiningen/stable/bin/lein
chmod u+x lein
sudo mv lein /usr/local/bin


### Configuration

Before we run any kbase sdk based service, we need to satisfy a small set of requirements to supply configuration to it. 

- one configuration file containing service endpoints and service configuration properties
- two environment variables, one to point to the config file, the other to provide the name of the service.

#### Configuration

##### Config data file

The sdk supports a process for generating a per-deployment configuration file during server startup. In production, the entrypoint script should produce this configuration file, in a format suitable to the server application itself, given a configuration data source in "ini" format.

> Note to self: see if we can get a change request in for this. Ini format is crap.

For development we will be supplying our own configuration. Although we will be operating against the CI environment during development, there is SDK method at present to obtain a current copy of the CI config data provided to modules.

> Note: For kb-sdk users, this material presented here is implicit in the tools used. However, since this is an exercise in getting down the bare bones of SDK development, we want to understandthis.

To be most polite, one should ask a core SDK developer or sysops person to provide a sample of the config data file used in CI.

In developing this module, though, I used a technique to reveal the file. I created a small module which just returns the config data file it lives in ```/kb/deployment/config.properties``` in the container in which the module is run. 

A copy of this file, with additional properties added and private data redacted, can be found in ```samples/config.properties```.  

> Note that although this file is named like a Java properties file, it is in fact an INI file. The two formats are very similar, with one exception that INI files support sections. 

The data file you create should be placed in ```devdata/config.properties```

##### Config template

To generate the config file used by the module service, we need to take the config.properties file and create our own deploy configuration file. Although the convention is to name this file "deploy.cfg", implying also a INI format, since the path to this file is provided to your service in an environment variable which you populate, it is really up to you.

I would advise a format which is most naturally, reliably, and easily ingested by your code. A canonical example might be JSON for Javascript; EDN for Clojure; properties for Java.

> At present, since this project was initially modeling the precise behavior of traditional KBase SDK apps, it still uses a cfg format. This requires one extra dependency and a configuration transoformation step in the server, since cfg only represents data as strings.

Although not strictly required by the SDK, it is user-friendly to utlize a template to generate the config file. So, given a data file, a template, and a script which can populate the template given the data file, we produce the target config file.

The template itself is in Mustache format. There is no standard format for the template, as far as I know, but since the format is something of an interface to the sysops team, it should be readily understandable in case it needs tweaking. I chose Mustache since it is widely implemented and understood, and very simple.

The template lives in ```templates/deploy.template.cfg```

##### Generating the config file

The config file is generated by the module app itself. The code includes a main function in the *deploy.core* namespace. Since we use lein for development (not necessary when the code is deployed), we have a *deploy* profile set up to facilitate running the correct main function.

```bash
$ lein with-profile deploy run ./templates/deploy.template.cfg ./devdata/config.properties ./devdata/deploy.cfg
```

Note that we store the data in *devdata*. This is a special directory in which we can place arbitrary development-time files which will not be checked into the repo. The README.md is not ignored, however, so please don't delete it, and you may use it to add additional notes about files which may appear in the directory.

The generated config file will be placed in ```devdata/deploy.cfg```.

#### Environment Variables

Two environment variables are required in order to properly hook the service up the runtime environment in deployment, so we must utilize them when developing too.

The first is ```KB_DEPLOYMENT_CONFIG```, which contains the path to the deploy configuration file we created above. 

The second is ```KB_SERVICE_NAME```, which contains the "official", common name of the service. This is a spaceless string. It is the same name as used to name the module's spec file, and to name them module within the spec. It is also the name used to interact with the Service Wizard, and is used in other contexts as well. 

In our case, the service name is always "jgi_gateway_clj"

> Note: I haven't found a use for it within the service itself.

These environment variable setup would look like this in the terminal:

```bash
$ export KB_DEPLOYMENT_CONFIG=`pwd`/deploy.cfg
$ export KB_SERVICE_NAME=jgi_gateway_clj
```

#### Development Operation

On of the advantages of using Clojure is that it supports a very productive workflow for working on source while a server is running, and having those changes reflected immediately in the running server. This is possible due to the language itself, and its integration into the JVM, but also Leinengen's ring plugin provides the easy configuration and smooth operation:

The plugin is enabled in the project.clj file. To enable live server coding issue this command from the terminal:

```
$ lein with-profile dev ring server-headless
```

or 

```
$ lein with-profile server run 3000
```

The first will use the ring plugin; the second will simply run the server on port 3000.

#### Putting it all together

The entire shebang would look like this:

```bash
$ lein with-profile deploy run ./templates/deploy.template.cfg ./devdata/config.properties ./devdata/deploy.cfg
$ export KB_DEPLOYMENT_CONFIG=`pwd`/devdata/deploy.cfg
$ export KB_SERVICE_NAME=jgi_gateway_clj
$ lein with-profile dev ring server-headless
```

fortunately there is a script to do this for you

```bash
$ bash scripts/dev.sh
```

### Using the dev server

Okay, great! Now we have a live, editable module service running locally at localhost:3000/rpc. What now?

In my case I need to use the service in the context of kbase-ui, developing a search interface to jgi.

Generally you may be running local tests, developing services for the Narrative or UI, widgets in Narrative or UI, or simply developing the service while using a json-rpc client side tool for generating requests and inspecting responses.

At present kbase clients integrated into the Narrative and UI depend on a configuration file mapping core services to endpoints, and the service wizard core service for mapping dynamic services to endpoints.

In order to operate against our locally deployed service, which is a dynamic service, we need to override the service wizard mechanism.

#### Overriding the service wizard

We could simply code around this, by making direct calls to the service using the generic client (used for requests to core services) to make requests of our localost:3000 service. But this would be messy, and require code changes just to support working against the local service.

To provide the ability to override a service which is normally accessed through the dynamic service mechanism, we can simply define it in the services configuration, setting the service name to that of the module name.

The dynamic service client will check if a service configuration key exists like ```services.NAME.url```. If it does, it will use that value, otherwise, it will call the service wizard to fetch the current url, and use that.

<!-- > scratch below, I think

In the UI configuration, we can utilize the service wizard override configuration group. This setting, found in ```config/services.yml``` provides a service setting very similar to a core service, with the following differences:

- the top level key is serviceWizardOverride
- the first path component is ```devservices``` not ```services```
- the service name (the path component following devservices) is the module name
- any path components following are handled by the service itself.

For example:

```yaml
serviceWizardOverride:
    jgi_gateway_clj:
        url: https://{{ deploy.services.urlBase }}/devservices/jgi_gateway_clj/rpc
```

The service wizard client itself knows to look for the override configuration whenever it handles a request for a given module. -->

#### Operating behind a proxy

Since all service requests must operate over https, and since UI and Narrative development already requires and documents a proxy-within-vm development workflow, we will in fact be operating our development server inside the vm and inside the nginx proxy.

This is very easy to do, just requiring 

- setting up the service project alongside the ui or narrative
- installing jdk 8 inside the vm
- running the service inside the vm
- providing a reverse proxy stanza in the nginx configuration

Don't worry, it is all documented below.

##### Set up alongside kbase-ui or narrative

In the kbase-ui documentation, a proxy-vm workflow is described in detail. Instructions are provided for setting up the project directory and the vm. Let's assume you've done this.

You will need to clone the module repo in the development directory (dev), so that the module repo is a sister directory to kbase-ui, and in the same directory as the Vagrantfile. This ensures that the module directory is shared inside the vm.

##### Install JDK 8 in the vm

From the development directory, enter the vm and install JDK 8.

```bash
$ vagrant ssh
$ sudo add-apt-repository ppa:openjdk-r/ppa
$ sudo apt-get update
$ sudo apt-get install openjdk-8-jdk
$ sudo udpate-ca-certificates -f
```

Make sure the module works.

```bash
$ cd /vagrant/jgi_gateway_clj/clojure
$ bash ./scripts/dev.sh
```

## Deploying

### Make the server

```
% lein with-profile service,deploy uberjar
```

This makes the jar with the server, invoked via server.main, and the deploy tool, invoked via deploy.main

### Copy assets into the distribution 

Here is the manual process:

- the target directory is dist/module
- cp clojure/server/target/server-0.1.0-SNAPSHOT.jar dist/module/server.jar
- cp clojure/server/scripts/Makefile dist/module/Makefile
<!-- - copy clojure/server/scripts/run.sh dist/module/run.sh -->

> TODO: write a script for this

this will help find the project version, which we need in order to identify the correct uberjar file in target:

https://stackoverflow.com/questions/16270805/how-to-get-the-version-of-the-current-clojure-project-in-the-repl

We we need to do is create another clojure main script to handle preparation of the distrubution.


### Make a docker image

````
docker build -t jgi-gateway-clj .
```

Run it

```
docker run -rm -d -p 3000:3000 jgi-gateway-clj
```


Run it directly:

For server mode:

```
export KB_DEPLOYMENT_CONFIG=`pwd`/deploy.cfg
export KB_SERVICE_NAME=jgi_gateway_clj
java -cp server.jar server.core ${PORT}
```

For async mode:

```
java -cp server.jar server.core in.json out.json token
```

where

- in.json is the input parameters in json format
- out.json is the ouput in json format
- token is the auth token string provided directly

So now, for testing in async mode, we need to do this:

- build the docker container
- create directories with sample input and prepared sample output
- run the dockerized app against each of the test work dirs
- compare the real output to the expected.

```
docker build -t jgi-gateway-clj .
docker run --rm jgi-gateway-clj -v `pwd`/tests/work1:/kb/module/work async
```

## Module registration

module is built into an image during registration using the top level Dockerfile

> explain more

## Module lifecycle in server mode

When a module app is run in async mode or a module is run in service mode and accessed for the first time, the docker container wrapping it is run, invoking the entrypoint script.

This script is set in the Dockerfile used to create the image, and by convension is called ```./entrypoint.sh```. We will continue to use that since it is a well known convention.

### Deployment 

The first responsibility of the entrypoint script is to create the deployment configuration file. This is the heart of the reconfigurablity of modules; the configuration may be modified and the service restarted to rebuild the configuration file with new values.

The deployment script must create a configuration file understood by the module given a configuration data file in "ini" format.

The configuration data file contains both standard kbase properties, as well as properties specific to the module itself.

> NOTE: it is unclear how module-specific properties are made available and provided to the module, since it is not under control of the module author.

The "ini" format allows for a top-level section token, with properties within the section being flat (no nested properties.)

In the config data file, the section name is "global".

There are three ways of providing configuration to the deployment script:

- the config data file - provided as an argument to the deployment script
- environment variables:
  - KBASE_ENDPOINT
  - AUTH_SERVICE_URL
  - AUTH_SERVICE_URL_ALLOW_INSECURE
  - KBASE_SECURE_CONFIG_PARAM_<param>

#### The config data file

The config data file may be provided as the second argument to the deployment script. It is an ini file with the top level property "global". It contains required and optional configuration properties. The rules for what is required and optional is dependent upon which environment variables have been set.

This file, if present, will be named ```./work/config.properties```.

> Note: Although this is putatively a java "properties" file, the fact that at least the python ConfigParser is used, and it will throw an error if it consumes a file without a section (as ini files have), I can only surmise that this is not a true properties file, which cannot have a section.

If the file has been provided at all, it is used to set the following service urls, unless the corresponding KBASE_SECURE_CONFIG_PARAM_<param> has been set, then all is well. However, if it has not been set, then that service url will not be set and a service which relies upon it being set may fail.

Oh, and it needs to have these propertes set under the ```[global``` section:

The following properties may be set:

- kbase_endpoint
- job_service_url
- workspace_url
- shock_url
- handle_url
- srv_wiz_url
- njsw_url
- auth_service_url_allow_insecure
- auth_service_url

> Note that the requirement for a specific service setting depends upon whether the module requires that service url setting. If it is not required it should not be required in the configuration file template, nor in the codebase itself. 

> Note that depending upon the language and tools used to build the deploy config file, an error may or may not be generated if a required configuration key is present.

For instance, if shock_url is not provided, and KBASE_SECURE_CONFIG_PARAM_SHOCK_URL environment variable will be used in its stead.

#### The config data file not provided

If the config data file is not provided, the configration will be built from environment variables and certain pre-determined service paths:

- KBASE_ENDPOINT must be provided, as it will be used to construct the service endpoints

- The following service endpoints will be constructed:
    - job_service_url - KBASE_ENDPOINT/userandjobstate
    - workspace_url - KBASE_ENDPOINT/ws
    - shock_url - KBASE_ENDPOINT/shock-api
    - handle_url - KBASE_ENDPOINT/handle_service
    - srv_wiz_url - KBASE_ENDPOINT/service_wizard
    - njsw_url - KBASE_ENDPOINT/njs_wrapper

- If AUTH_SERVICE_URL is set, then auth_service_url will be set to its value
    - otherwise, what it is it set to?

- if AUTH_SERVICE_URL_ALLOW_INSECURE is set, auth_service_url_allow_insecure is set to its value, otherwise it is set to the string "false"

- Any KBASE_SECURE_CONFIG_PARAM_<param> properties are set, which may override any of the above, supply them if they are missing (i.e. the AUTH_ settings), or set values not specified here (e.g. module-specific values)


### Building config file

The configuration file itself, the data file known to the module, is built by using the values provided above to populate a template file. The template file may be implemented in a templating system well supported by the module language, but usage of a common format is preferred. For instance, Python modules by default use Jinja templates, but this is a Python-only format (although ported to some other languages.) On the other hand, a very-widespread format like Mustache or Handlebars would be preferable.

The template file may also result in a data file format suitable to the language. It is notable that the ini format is not necessarily suitable for data to be consumed by a module. For instance, it only represents values as strings, which puts the burden on the module itself to convert numbers and boolean values. It is probably better to have the deployment script do this interpretation and populate a proper data structure, failing if any values are invalid.

For clojure the template language is mustache (using the stencil library) and the data format is edn, the native data serialization format for Clojure.

The config file is built by applying a deployment script or application to the template, with data applied (as described above). The default script will rename the original template, and write the new one in its place. I prefer, though, to keep the original one as-is, and write a new file.

I prefer this because otherwise the service may inadvertently be started with an invalid config file, which would supply bad values to it. Better to just have the file be missing if it has not been processed yet, or if the config file generation fails.

Again, since this is under control of the module, this should be no problem.


Optional


It is expected to start the service as either a json-rpc service or as a one-shot command line app. 

The different startup modes are

server
async
init
test
bash
report

### server

Server mode is indicated by lack of argument provided to docker run. By convention the ```./start_server.sh``` script is called.

By convension, the environment variable ```KB_DEPLOYMENT_CONFIG``` is set to the location of the config file, which is by convention located in the parent directory to the module (i.e. in /kb).

> The term "by convention" implies that these values are not predetermined by the SDK; rather they are commonly used, and for sdk-generated apps will be set this way. However, the values themselves and their usage within the module are determined the module's deployment script, the entrypoint script, and the module code -- that is, they are under complete control of the module itself.

### async

Async mode is indicated by the string "async" being provided to docker run. In this case the predetermined file ./work/token is made available. By convention the entrypoint script will set the environment variable ```KB_AUTH_TOKEN``` to this value

When the container is started, files are mounted in a common location:

/work/token

## Testing

### Testing deployment


java -cp ./target/server-0.1.0-SNAPSHOT-standalone.jar deploy.core ./test/deploy.template.cfg ./test/config.properties ./test/deploy.cfg

## License

Copyright © 2016 FIXME

Distributed under the Eclipse Public License either version 1.0 or (at
your option) any later version.
